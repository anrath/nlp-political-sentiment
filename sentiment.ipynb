{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The following sample code was produced in part using GPT-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.nn.functional import softmax\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilingual Sentiment Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.185\n",
      "3.7266\n"
     ]
    }
   ],
   "source": [
    "class SentimentClassifier:\n",
    "    def __init__(self, model_name):\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "        self.model = BertForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "    def classify(self, text):\n",
    "        # Tokenize the input text and obtain model outputs\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs).logits\n",
    "            probs = softmax(outputs, dim=1)\n",
    "            rating = torch.dot(probs.view(-1), torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0])).item()\n",
    "            # label = torch.argmax(probs, dim=1).item()\n",
    "\n",
    "        return np.round(rating, 4)\n",
    "\n",
    "classifier = SentimentClassifier(\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "\n",
    "# Sample\n",
    "text_english = \"The course was well structured.\"\n",
    "text_spanish = \"El curso estuvo bien estructurado.\"\n",
    "print(classifier.classify(text_english))\n",
    "print(classifier.classify(text_spanish))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LM Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████| 1.47k/1.47k [00:00<00:00, 5.51MB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 312M/312M [00:19<00:00, 16.4MB/s] \n",
      "Downloading (…)neration_config.json: 100%|██████████| 293/293 [00:00<00:00, 1.08MB/s]\n",
      "Downloading (…)olve/main/source.spm: 100%|██████████| 802k/802k [00:00<00:00, 20.8MB/s]\n",
      "Downloading (…)olve/main/target.spm: 100%|██████████| 826k/826k [00:00<00:00, 20.9MB/s]\n",
      "Downloading (…)olve/main/vocab.json: 100%|██████████| 1.59M/1.59M [00:00<00:00, 19.4MB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 44.0/44.0 [00:00<00:00, 226kB/s]\n",
      "/home/anrath/miniconda3/envs/fastchat/lib/python3.10/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 1.44k/1.44k [00:00<00:00, 8.27MB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 312M/312M [00:19<00:00, 16.2MB/s] \n",
      "Downloading (…)neration_config.json: 100%|██████████| 293/293 [00:00<00:00, 1.63MB/s]\n",
      "Downloading (…)olve/main/source.spm: 100%|██████████| 826k/826k [00:00<00:00, 19.0MB/s]\n",
      "Downloading (…)olve/main/target.spm: 100%|██████████| 802k/802k [00:00<00:00, 20.8MB/s]\n",
      "Downloading (…)olve/main/vocab.json: 100%|██████████| 1.59M/1.59M [00:00<00:00, 19.9MB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 44.0/44.0 [00:00<00:00, 168kB/s]\n",
      "/home/anrath/miniconda3/envs/fastchat/lib/python3.10/site-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (512) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hola, ¿cómo estás?\n",
      "Hey, how are you?\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries from Hugging Face's transformers\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "class TranslationService:\n",
    "    def __init__(self):\n",
    "        # Initialize the English-to-Spanish model and tokenizer\n",
    "        self.en_to_es_model = MarianMTModel.from_pretrained(\"Helsinki-NLP/opus-mt-en-es\")\n",
    "        self.en_to_es_tokenizer = MarianTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-es\")\n",
    "\n",
    "        # Initialize the Spanish-to-English model and tokenizer\n",
    "        self.es_to_en_model = MarianMTModel.from_pretrained(\"Helsinki-NLP/opus-mt-es-en\")\n",
    "        self.es_to_en_tokenizer = MarianTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-es-en\")\n",
    "\n",
    "    def translate(self, text, target_language=\"es\"):\n",
    "        \"\"\"Perform the translation.\n",
    "\n",
    "        Args:\n",
    "        - text (str): Input text to be translated.\n",
    "        - target_language (str): Target language for translation. \"es\" for English-to-Spanish, \"en\" for Spanish-to-English.\n",
    "\n",
    "        Returns:\n",
    "        - str: Translated text.\n",
    "        \"\"\"\n",
    "        if target_language == \"es\":\n",
    "            # Translate English-to-Spanish\n",
    "            tokenized_text = self.en_to_es_tokenizer(text, return_tensors=\"pt\")\n",
    "            translation = self.en_to_es_model.generate(**tokenized_text)\n",
    "            return self.en_to_es_tokenizer.decode(translation[0], skip_special_tokens=True)\n",
    "        elif target_language == \"en\":\n",
    "            # Translate Spanish-to-English\n",
    "            tokenized_text = self.es_to_en_tokenizer(text, return_tensors=\"pt\")\n",
    "            translation = self.es_to_en_model.generate(**tokenized_text)\n",
    "            return self.es_to_en_tokenizer.decode(translation[0], skip_special_tokens=True)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid target language. Choose 'es' or 'en'.\")\n",
    "\n",
    "# Example usage\n",
    "service = TranslationService()\n",
    "translated_text_es = service.translate(\"Hello, how are you?\", target_language=\"es\")\n",
    "print(translated_text_es)  # Expected output: \"Hola, ¿cómo estás?\"\n",
    "translated_text_en = service.translate(\"Hola, ¿cómo estás?\", target_language=\"en\")\n",
    "print(translated_text_en)  # Expected output: \"Hello, how are you?\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T5 (LLM) Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████| 1.21k/1.21k [00:00<00:00, 7.22MB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 242M/242M [00:14<00:00, 16.2MB/s] \n",
      "Downloading (…)neration_config.json: 100%|██████████| 147/147 [00:00<00:00, 513kB/s]\n",
      "Downloading (…)ve/main/spiece.model: 100%|██████████| 792k/792k [00:00<00:00, 13.0MB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 2.32k/2.32k [00:00<00:00, 9.11MB/s]\n",
      "/home/anrath/miniconda3/envs/fastchat/lib/python3.10/site-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English to Spanish: en to es: Hello, how are you?\n",
      "Spanish to English: es to en: Hola, cómo estás\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "class T5Translator:\n",
    "    def __init__(self, model_name: str = \"t5-small\"):\n",
    "        \"\"\"\n",
    "        Initialize the T5 Translator with the specified model.\n",
    "        \n",
    "        Args:\n",
    "            model_name (str): Name of the T5 model. Default is \"t5-small\".\n",
    "        \"\"\"\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(model_name).to(self.device)\n",
    "        self.tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "        \n",
    "    def translate(self, text: str, source_language: str, target_language: str) -> str:\n",
    "        \"\"\"\n",
    "        Translate text from the source language to the target language.\n",
    "        \n",
    "        Args:\n",
    "            text (str): The input text to be translated.\n",
    "            source_language (str): The source language code (e.g., \"en\" for English).\n",
    "            target_language (str): The target language code (e.g., \"es\" for Spanish).\n",
    "            \n",
    "        Returns:\n",
    "            str: The translated text.\n",
    "        \"\"\"\n",
    "        # Prepare the prompt text for the T5 model. For example: \"translate English to Spanish: Hello\"\n",
    "        prompt = f\"translate {source_language} to {target_language}: {text}\"\n",
    "        \n",
    "        # Tokenize the prompt text\n",
    "        inputs = self.tokenizer.encode(prompt, return_tensors=\"pt\", max_length=512, truncation=True).to(self.device)\n",
    "        \n",
    "        # Generate the translated text\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(inputs)\n",
    "        \n",
    "        # Decode the translated text\n",
    "        translated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        return translated_text\n",
    "\n",
    "# Instantiate the T5Translator\n",
    "translator = T5Translator()\n",
    "\n",
    "# Translate English to Spanish\n",
    "english_text = \"Hello, how are you?\"\n",
    "spanish_translation = translator.translate(english_text, \"en\", \"es\")\n",
    "print(f\"English to Spanish: {spanish_translation}\")\n",
    "\n",
    "# Translate Spanish to English\n",
    "spanish_text = \"Hola, ¿cómo estás?\"\n",
    "english_translation = translator.translate(spanish_text, \"es\", \"en\")\n",
    "print(f\"Spanish to English: {english_translation}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilingual Sentence Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)31d34/.gitattributes: 100%|██████████| 345/345 [00:00<00:00, 2.15MB/s]\n",
      "Downloading (…)_Pooling/config.json: 100%|██████████| 190/190 [00:00<00:00, 853kB/s]\n",
      "Downloading (…)e4a1a31d34/README.md: 100%|██████████| 3.74k/3.74k [00:00<00:00, 21.8MB/s]\n",
      "Downloading (…)a1a31d34/config.json: 100%|██████████| 718/718 [00:00<00:00, 3.89MB/s]\n",
      "Downloading (…)ce_transformers.json: 100%|██████████| 122/122 [00:00<00:00, 454kB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 1.11G/1.11G [00:59<00:00, 18.6MB/s]\n",
      "Downloading (…)nce_bert_config.json: 100%|██████████| 53.0/53.0 [00:00<00:00, 171kB/s]\n",
      "Downloading (…)tencepiece.bpe.model: 100%|██████████| 5.07M/5.07M [00:00<00:00, 16.3MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 150/150 [00:00<00:00, 425kB/s]\n",
      "Downloading (…)31d34/tokenizer.json: 100%|██████████| 9.10M/9.10M [00:00<00:00, 19.0MB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 550/550 [00:00<00:00, 1.98MB/s]\n",
      "Downloading (…)1a31d34/modules.json: 100%|██████████| 229/229 [00:00<00:00, 317kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity score between the English and Spanish sentences: 0.9509\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "\n",
    "# 2. Load the multilingual Sentence Transformer model\n",
    "model = SentenceTransformer('paraphrase-xlm-r-multilingual-v1')\n",
    "\n",
    "def compute_similarity(sentence1: str, sentence2: str) -> float:\n",
    "    \"\"\"\n",
    "    Computes the semantic similarity between two sentences using cosine similarity.\n",
    "\n",
    "    Parameters:\n",
    "    - sentence1 (str): The first sentence.\n",
    "    - sentence2 (str): The second sentence.\n",
    "\n",
    "    Returns:\n",
    "    - float: The cosine similarity score between the two sentences.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 3. Compute embeddings for both sentences\n",
    "    embedding1 = model.encode(sentence1, convert_to_tensor=True)\n",
    "    embedding2 = model.encode(sentence2, convert_to_tensor=True)\n",
    "    \n",
    "    # 4. Compute cosine similarity\n",
    "    cosine_similarity = util.pytorch_cos_sim(embedding1, embedding2).item()\n",
    "\n",
    "    return cosine_similarity\n",
    "\n",
    "# Example usage:\n",
    "english_sentence = \"I love programming\"\n",
    "spanish_sentence = \"Me encanta programar\"\n",
    "similarity_score = compute_similarity(english_sentence, spanish_sentence)\n",
    "print(f\"Similarity score between the English and Spanish sentences: {similarity_score:.4f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
